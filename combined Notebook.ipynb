{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f007275",
   "metadata": {},
   "source": [
    "### Approach\n",
    "* In Task 2, we can combine all 3 datasets: **Emoticons dataset**, **Features dataset** and **Text sequence dataset**.\n",
    "* Our initial idea was to use the 1st dataset to find the digit encodings of each emoticon in the 3rd dataset. In that way, we can extract better features from the 3rd datasaet. But, it was difficult to come up with a working algorithm.\n",
    "* Here, we have trained an LSTM classifier on the 1st dataset, TCN classifier on the 2nd dataset and LSTM classifier on the 3rd dataset. We have reduced the parameters in each model, considerably to ensure the 10,000 parameter limit.\n",
    "* We have implemented a **\"Majority Voting\" algorithm weighted by Validation accuracies** of the individual models.\n",
    "* The combined model is performing slightly worse than the best individual model, the TCN classifier for the 2nd dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3649ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30621003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read emoticons dataset\n",
    "train_emoticon_df = pd.read_csv(\"datasets/train/train_emoticon.csv\")\n",
    "train_emoticon_X = train_emoticon_df['input_emoticon'].tolist()\n",
    "train_emoticon_Y = train_emoticon_df['label'].tolist()\n",
    "\n",
    "valid_emoticon_df = pd.read_csv(\"datasets/valid/valid_emoticon.csv\")\n",
    "valid_emoticon_X = valid_emoticon_df['input_emoticon'].tolist()\n",
    "valid_emoticon_Y = valid_emoticon_df['label'].tolist()\n",
    "\n",
    "test_emoticon_X = pd.read_csv(\"datasets/test/test_emoticon.csv\")['input_emoticon'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a04c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read feature dataset\n",
    "train_feat = np.load(\"datasets/train/train_feature.npz\", allow_pickle=True)\n",
    "train_feat_X = train_feat['features']\n",
    "train_feat_Y = train_feat['label']\n",
    "\n",
    "valid_feat = np.load(\"datasets/valid/valid_feature.npz\", allow_pickle=True)\n",
    "valid_feat_X = valid_feat['features']\n",
    "valid_feat_Y = valid_feat['label']\n",
    "\n",
    "test_feat_X = np.load(\"datasets/test/test_feature.npz\", allow_pickle=True)['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fd4db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text sequence dataset\n",
    "train_seq_df = pd.read_csv(\"datasets/train/train_text_seq.csv\")\n",
    "train_seq_X = train_seq_df['input_str'].tolist()\n",
    "train_seq_Y = train_seq_df['label'].tolist()\n",
    "\n",
    "valid_seq_df = pd.read_csv(\"datasets/valid/valid_text_seq.csv\")\n",
    "valid_seq_X = valid_seq_df['input_str'].tolist()\n",
    "valid_seq_Y = valid_seq_df['label'].tolist()\n",
    "\n",
    "test_seq_X = pd.read_csv(\"datasets/test/test_text_seq.csv\")['input_str'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78d004",
   "metadata": {},
   "source": [
    "### Dataset 1\n",
    "Firstly, we get the frequency of the emojis present across the train, validation and test datasets. There are a total of 226 unique emojis. \\\n",
    "The `EmoticonLSTMClassifier` processes sequences of emojis, converting them into dense embeddings and using an **LSTM** to capture temporal patterns. The model outputs a probability distribution over two classes and is trained using cross-entropy loss. The ultimate goal is to classify emoji sequences into one of two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43b42f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70a2a848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoticons = dict()\n",
    "for seq in train_emoticon_X:\n",
    "    for emo in seq:\n",
    "        emoticons[emo] = emoticons.get(emo,0) + 1\n",
    "for seq in valid_emoticon_X:\n",
    "    for emo in seq:\n",
    "        emoticons[emo] = emoticons.get(emo,0) + 1\n",
    "for seq in test_emoticon_X:\n",
    "    for emo in seq:\n",
    "        emoticons[emo] = emoticons.get(emo,0) + 1\n",
    "num_emoticons = len(emoticons) # 214\n",
    "num_emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10c64f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of 226 unique emojis\n",
    "emoji_list = list(emoticons.keys()) # Add all 226 emojis here\n",
    "\n",
    "# Create a dictionary to map each emoji to a unique integer\n",
    "emoji_to_idx = {emoji: idx for idx, emoji in enumerate(emoji_list)}\n",
    "\n",
    "# Convert an emoticon string to a sequence of indices (based on the 214 emoji vocabulary)\n",
    "def emoticon_to_idx_seq(emoticon_str):\n",
    "    return [emoji_to_idx[emoji] for emoji in emoticon_str]\n",
    "\n",
    "# Prepare your dataset (example dataset creation)\n",
    "# X = [\"ðŸ™‚ðŸ˜‚ðŸ˜›ðŸ™ðŸ˜ŠðŸ˜ðŸ¥³ðŸ˜¢ðŸ˜ŽðŸ¤”ðŸ˜ðŸ˜‹ðŸ˜”\", \"ðŸ˜žðŸ˜ŠðŸ¥³ðŸ™‚ðŸ™ðŸ˜ŽðŸ˜›ðŸ˜‚ðŸ˜ðŸ˜¢ðŸ˜”ðŸ˜‹ðŸ¤”\", ...]  # List of strings (sequence of 13 emojis)\n",
    "# y = [0, 1, ...]  # Binary labels\n",
    "\n",
    "# Convert emoticon strings to sequences of indices\n",
    "train_X_idx = [emoticon_to_idx_seq(s) for s in train_emoticon_X]\n",
    "valid_X_idx = [emoticon_to_idx_seq(s) for s in valid_emoticon_X]\n",
    "test_X_idx = [emoticon_to_idx_seq(s) for s in test_emoticon_X]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_X = torch.tensor(train_X_idx, dtype=torch.long)\n",
    "train_y = torch.tensor(train_emoticon_Y, dtype=torch.long)\n",
    "\n",
    "valid_X_1 = torch.tensor(valid_X_idx, dtype=torch.long)\n",
    "valid_y_1 = torch.tensor(valid_emoticon_Y, dtype=torch.long)\n",
    "\n",
    "test_X_1 = torch.tensor(test_X_idx, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41cc3d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of `Emoticon LSTM Classifier`: 91.41%\n"
     ]
    }
   ],
   "source": [
    "class EmoticonLSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=num_emoticons, embedding_dim=4, hidden_dim=20, output_size=2, batch_size=16, n_epochs=20, learning_rate=0.0015, n_layers=1):\n",
    "        super(EmoticonLSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # Store hyperparameters as instance variables\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        # Loss and optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(embedded)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Get output from the last time step\n",
    "        return self.softmax(out)\n",
    "    \n",
    "    # Training\n",
    "    def train_model(self, dataloader):\n",
    "        # Training loop\n",
    "        for epoch in range(self.n_epochs):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # print(f'Epoch [{epoch+1}/{self.n_epochs}], Loss: {total_loss/len(dataloader):.4f}')\n",
    "\n",
    "    # Evaluation\n",
    "    def evaluate_model(self, dataloader):\n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                outputs = self(batch_X)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "        return (correct / total * 100)\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = num_emoticons\n",
    "embedding_dim = 4\n",
    "hidden_dim = 18\n",
    "output_size = 2  # Binary classification\n",
    "n_layers = 1\n",
    "batch_size = 16\n",
    "n_epochs = 20\n",
    "learning_rate = 0.0015\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataset = TensorDataset(train_X, train_y) # train_X_unicode\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataset = TensorDataset(valid_X_1, valid_y_1)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model\n",
    "model_1 = EmoticonLSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_size, batch_size, n_epochs, learning_rate, n_layers)\n",
    "\n",
    "model_1.train_model(train_loader)\n",
    "\n",
    "val_accuracy_1 = model_1.evaluate_model(valid_loader)\n",
    "print(f'Validation Accuracy of `Emoticon LSTM Classifier`: {val_accuracy_1:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "160c26eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "EmoticonLSTMClassifier                   --\n",
       "â”œâ”€Embedding: 1-1                         904\n",
       "â”œâ”€LSTM: 1-2                              1,728\n",
       "â”œâ”€Linear: 1-3                            38\n",
       "â”œâ”€Softmax: 1-4                           --\n",
       "â”œâ”€CrossEntropyLoss: 1-5                  --\n",
       "=================================================================\n",
       "Total params: 2,670\n",
       "Trainable params: 2,670\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install torchinfo\n",
    "from torchinfo import summary\n",
    "summary(model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651db690",
   "metadata": {},
   "source": [
    "### Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2031e9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 7, 12]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# many columns have unique value\n",
    "num_examples, num_vectors, dim_vectors = train_feat_X.shape\n",
    "useful_cols = [i for i in range(num_vectors) if max([pd.Series(train_feat_X[:,i,j]).nunique() for j in range(dim_vectors)]) > 1]\n",
    "useful_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed9ca021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b23b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chomp1d layer\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size]\n",
    "\n",
    "# TemporalBlock layer\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size, stride=stride,\n",
    "                               padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size, stride=stride,\n",
    "                               padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "# TemporalConvNet layer\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# TCNClassifier class with encapsulated methods\n",
    "class TCNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_channels, num_classes=2, kernel_size=2, dropout=0.2, lr=0.001):\n",
    "        super(TCNClassifier, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.fc = nn.Linear(num_channels[-1], num_classes)\n",
    "\n",
    "        # Loss function and optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TCN expects input shape: (batch_size, input_size, sequence_length)\n",
    "        y = self.tcn(x)\n",
    "        # y[:, :, -1] takes the output from the last time step of the sequence\n",
    "        return self.fc(y[:, :, -1])\n",
    "\n",
    "    # Method to prepare dataloader\n",
    "    def prepare_dataloader(self, X, y, batch_size=32):\n",
    "        dataset = TensorDataset(X, y)\n",
    "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Method to train the model\n",
    "    def train_model(self, dataloader, num_epochs=10):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in dataloader:\n",
    "                # Move tensors to the appropriate device\n",
    "                inputs = inputs.permute(0, 2, 1)  # Change shape to (batch_size, input_size, sequence_length)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n",
    "\n",
    "    # Method to evaluate the model\n",
    "    def evaluate_model(self, dataloader):\n",
    "        self.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.permute(0, 2, 1)\n",
    "                outputs = self.forward(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = (100 * correct / total)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "834d9d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_vectors = 3\n",
    "vector_dim = int(768*1/3)\n",
    "n_samples = 7080\n",
    "num_channels = [2, 16]  # Number of channels for each layer in the TCN\n",
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "learning_rate = 0.0015\n",
    "\n",
    "\n",
    "train_X = torch.from_numpy(train_feat_X[ : , useful_cols, : vector_dim]).float()\n",
    "train_y = torch.from_numpy(train_feat_Y).long()\n",
    "\n",
    "valid_X_2 = torch.from_numpy(valid_feat_X[ : , useful_cols, : vector_dim]).float()\n",
    "valid_y_2 = torch.from_numpy(valid_feat_Y).long()\n",
    "\n",
    "test_X_2 = torch.from_numpy(test_feat_X[ : , useful_cols, : vector_dim]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6160ca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of `TCN Feature Classifier`: 96.93%\n"
     ]
    }
   ],
   "source": [
    "# Initialize models and dataloaders\n",
    "model_2 = TCNClassifier(input_size=vector_dim, num_channels=num_channels, lr=learning_rate)\n",
    "train_loader = model_2.prepare_dataloader(train_X, train_y, batch_size)\n",
    "valid_loader = model_2.prepare_dataloader(valid_X_2, valid_y_2, batch_size)\n",
    "\n",
    "# Train the models\n",
    "model_2.train_model(train_loader, num_epochs)\n",
    "while model_2.evaluate_model(valid_loader)<93:\n",
    "    model_2 = TCNClassifier(input_size=vector_dim, num_channels=num_channels, lr=learning_rate)\n",
    "    model_2.train_model(train_loader, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "val_accuracy_2 = model_2.evaluate_model(valid_loader)\n",
    "\n",
    "print(f'Validation Accuracy of `TCN Feature Classifier`: {val_accuracy_2:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4df65554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "TCNClassifier                            --\n",
       "â”œâ”€TemporalConvNet: 1-1                   --\n",
       "â”‚    â””â”€Sequential: 2-1                   --\n",
       "â”‚    â”‚    â””â”€TemporalBlock: 3-1           1,550\n",
       "â”‚    â”‚    â””â”€TemporalBlock: 3-2           656\n",
       "â”œâ”€Linear: 1-2                            34\n",
       "â”œâ”€CrossEntropyLoss: 1-3                  --\n",
       "=================================================================\n",
       "Total params: 3,884\n",
       "Trainable params: 3,884\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install torchinfo\n",
    "from torchinfo import summary\n",
    "summary(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d56a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0be1e404",
   "metadata": {},
   "source": [
    "### Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddc19e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class LSTMClassifierWithEmbedding(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_size, output_size):\n",
    "        super(LSTMClassifierWithEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=input_size, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Input is (batch_size, seq_len) and output is (batch_size, seq_len, embedding_dim)\n",
    "        _, (hn, _) = self.lstm(x)  # hn is the hidden state at the last time step\n",
    "        out = self.fc(hn[-1])  # hn[-1] is the hidden state of the last LSTM layer\n",
    "        return out\n",
    "\n",
    "    def train_model(self, train_loader, criterion, optimizer, num_epochs):\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()  # Set the model to training mode\n",
    "            epoch_loss = 0  # To track the loss for each epoch\n",
    "\n",
    "            for batch_sequences, batch_labels in train_loader:\n",
    "                # Forward pass\n",
    "                outputs = self(batch_sequences)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss / len(train_loader):.4f}')\n",
    "\n",
    "    def evaluate_model(self, val_loader):\n",
    "        self.eval()  # Set the model to evaluation mode\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_sequences, val_labels in val_loader:\n",
    "                outputs = self(val_sequences)\n",
    "                predictions = torch.sigmoid(outputs).round()  # Convert logits to binary predictions (0 or 1)\n",
    "                all_labels.extend(val_labels.cpu().numpy())\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d942ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sequence of digits into tensor (just a list of numbers)\n",
    "def digit_sequence_to_tensor(sequence):\n",
    "    return torch.tensor([int(digit) for digit in sequence], dtype=torch.long)\n",
    "\n",
    "# Converting to Ytorch Tensors\n",
    "train_X = [digit_sequence_to_tensor(seq) for seq in train_seq_X]\n",
    "train_X = torch.stack(train_X)  # Stack to form a batch tensor\n",
    "train_y = torch.tensor(train_seq_Y).float().unsqueeze(1)  # Convert labels to tensor and reshape\n",
    "\n",
    "valid_X_3 = [digit_sequence_to_tensor(seq) for seq in valid_seq_X]\n",
    "valid_X_3 = torch.stack(valid_X_3)  # Stack to form a batch tensor\n",
    "valid_y_3 = torch.tensor(valid_seq_Y).float().unsqueeze(1)\n",
    "\n",
    "test_X_3 = [digit_sequence_to_tensor(seq) for seq in test_seq_X]\n",
    "test_X_3 = torch.stack(test_X_3)  # Stack to form a batch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f543c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 85.07%\n"
     ]
    }
   ],
   "source": [
    "# Assuming sequences, labels, val_sequences, val_labels, etc. are already defined\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 10       # Number of possible digits (0-9)\n",
    "embedding_dim = 16     # Size of the embedding vector for each digit\n",
    "hidden_size = 20       # Size of the LSTM hidden state\n",
    "output_size = 1        # Binary classification (0 or 1)\n",
    "learning_rate = 0.0015\n",
    "num_epochs = 25\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "train_dataset = TensorDataset(train_X, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(valid_X_3, valid_y_3)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Initialize the LSTM model\n",
    "model_3 = LSTMClassifierWithEmbedding(input_size, embedding_dim, hidden_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross Entropy with Logits\n",
    "optimizer = optim.Adam(model_3.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "model_3.train_model(train_loader, criterion, optimizer, num_epochs)\n",
    "\n",
    "# Evaluate the model\n",
    "val_accuracy_3 = model_3.evaluate_model(val_loader) * 100\n",
    "print(f\"Validation Accuracy: {val_accuracy_3:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fc63c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "LSTMClassifierWithEmbedding              --\n",
       "â”œâ”€Embedding: 1-1                         160\n",
       "â”œâ”€LSTM: 1-2                              3,040\n",
       "â”œâ”€Linear: 1-3                            21\n",
       "=================================================================\n",
       "Total params: 3,221\n",
       "Trainable params: 3,221\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c4f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e252d601",
   "metadata": {},
   "source": [
    "### Combined Validation Accuracy Weighted Majority Vote Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62af9931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of the combined voting model: 93.66%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the combined voting model\n",
    "def evaluate_combined_model(model_1, model_2, model_3):\n",
    "    model_1.eval()\n",
    "    model_2.eval()\n",
    "    model_3.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(len(valid_X_1)):\n",
    "        with torch.no_grad():\n",
    "            o1 = model_1(valid_X_1[i].unsqueeze(0))  # LSTM output\n",
    "            _, p1 = torch.max(o1, 1)\n",
    "            \n",
    "            # Permuting the input to match TCN input shape: (batch_size, input_size, sequence_length)\n",
    "            o2 = model_2(valid_X_2[i].unsqueeze(0).permute(0, 2, 1))\n",
    "            _, p2 = torch.max(o2, 1)\n",
    "            \n",
    "            o3 = model_3(valid_X_3[i].unsqueeze(0))\n",
    "            _, p3 = torch.max(o3, 1)\n",
    "            \n",
    "            # Simple majority vote calculation\n",
    "            # votes = torch.tensor([p1.item(), p2.item(), p3.item()])\n",
    "            # vote = torch.mode(votes)[0].item()  # Taking the majority vote\n",
    "            \n",
    "            # Convert accuracies to weights (you could normalize them if needed)\n",
    "            weights = torch.tensor([val_accuracy_1 / 100, val_accuracy_2 / 100, val_accuracy_3 / 100])\n",
    "            votes = torch.tensor([p1.item(), p2.item(), p3.item()])  # p1, p2, p3 are model predictions\n",
    "            weighted_votes = votes.float() * weights\n",
    "            vote = 1 if weighted_votes.sum() >= (weights.sum() / 2) else 0  # Compare to half the total weight for majority\n",
    "            \n",
    "            total += 1\n",
    "            correct += (vote == valid_y_1[i].item())  # Correctly index valid_y[i]\n",
    "    \n",
    "    return correct / total * 100\n",
    "\n",
    "print(f'Validation Accuracy of the combined voting model: {evaluate_combined_model(model_1, model_2, model_3):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "927b9917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to calculate the number of parameters\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d095532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of learned parameters of `Emoticon LSTM Classifier`      :  2670\n",
      "Total number of learned parameters of `TCN Feature Classifier`        :  3884\n",
      "Total number of learned parameters of `Text Sequence LSTM Classifier` :  3221\n",
      "=================================================================================\n",
      "Total number of learned parameters of the combined model              :  9775\n",
      "=================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of learned parameters of `Emoticon LSTM Classifier`      : \", 2670)\n",
    "print(\"Total number of learned parameters of `TCN Feature Classifier`        : \", 3884)\n",
    "print(\"Total number of learned parameters of `Text Sequence LSTM Classifier` : \", 3221)\n",
    "print(\"=================================================================================\")\n",
    "cnt = 2670 + 3884 + 3221\n",
    "print(\"Total number of learned parameters of the combined model              : \", cnt)\n",
    "print(\"=================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550cca25",
   "metadata": {},
   "source": [
    "### Generating and Saving Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "662f6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.eval()\n",
    "model_2.eval()\n",
    "model_3.eval()\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = []\n",
    "\n",
    "for i in range(len(test_X_1)):\n",
    "    with torch.no_grad():\n",
    "        o1 = model_1(test_X_1[i].unsqueeze(0))  # LSTM output\n",
    "        _, p1 = torch.max(o1, 1)\n",
    "\n",
    "        # Permuting the input to match TCN input shape: (batch_size, input_size, sequence_length)\n",
    "        o2 = model_2(test_X_2[i].unsqueeze(0).permute(0, 2, 1))\n",
    "        _, p2 = torch.max(o2, 1)\n",
    "\n",
    "        o3 = model_3(test_X_3[i].unsqueeze(0))\n",
    "        _, p3 = torch.max(o3, 1)\n",
    "        \n",
    "        # Convert accuracies to weights (you could normalize them if needed)\n",
    "        weights = torch.tensor([val_accuracy_1 / 100, val_accuracy_2 / 100, val_accuracy_3 / 100])\n",
    "        votes = torch.tensor([p1.item(), p2.item(), p3.item()])  # p1, p2, p3 are model predictions\n",
    "        weighted_votes = votes.float() * weights\n",
    "        vote = 1 if weighted_votes.sum() >= (weights.sum() / 2) else 0  # Compare to half the total weight for majority\n",
    "        \n",
    "        predictions.append(vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ebf1e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to pred_combined.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the predictions to a text file\n",
    "with open('pred_combined.txt', 'w') as f:\n",
    "    for prediction in predictions:\n",
    "        f.write(f'{prediction}\\n')\n",
    "\n",
    "print(f'Predictions saved to pred_combined.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dfae6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
